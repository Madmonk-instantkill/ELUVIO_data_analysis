{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Eluvio project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mp7u6Wk_N7tn",
        "outputId": "a18d8074-9a86-40be-e940-bfa609597e06"
      },
      "source": [
        "#install Java8\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n",
        "\n",
        "# Unzip the spark file to the current folder\n",
        "!tar xf spark-3.0.0-bin-hadoop3.2.tgz\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop3.2\"\n",
        "\n",
        "# Install findspark using pip\n",
        "!pip install -q findspark\n",
        "\n",
        "# Spark for Python\n",
        "!pip install pyspark\n",
        "import findspark\n",
        "findspark.init(\"/content/spark-3.0.0-bin-hadoop3.2\")\n",
        "import pyspark"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/b0/9d6860891ab14a39d4bddf80ba26ce51c2f9dc4805e5c6978ac0472c120a/pyspark-3.1.1.tar.gz (212.3MB)\n",
            "\u001b[K     |████████████████████████████████| 212.3MB 72kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 52.4MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.1-py2.py3-none-any.whl size=212767604 sha256=2f627f0785ace15402b087accefcb7cbab776358cba316a7093d1f18bdfb005d\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/90/c0/01de724414ef122bd05f056541fb6a0ecf47c7ca655f8b3c0f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJqSI2-p0uLP"
      },
      "source": [
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark=SparkSession.builder.appName(\"Coding_challenge\").getOrCreate()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNIqZ_DC0uNv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c29e0c0c-83f2-4ef4-ef08-1b0851b1f0a4"
      },
      "source": [
        "#from pyspark.sql import SparkSession\n",
        "#spark=SparkSession.builder.appName(\"work\").getOrCreate()\n",
        "\n",
        "from google.colab import drive\n",
        "import os.path\n",
        "from os import path\n",
        "\n",
        "#######note : if error :file exists occurs , please comment the immediate  below line ie os.mkdir('/content/eluvio_data')\n",
        "\n",
        "os.mkdir('/content/eluvio_data')\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "!pip install pyunpack\n",
        "!pip install patool\n",
        "from pyunpack import Archive\n",
        "Archive('/content/gdrive/MyDrive/Eluvio_DS_Challenge.rar').extractall('/content/eluvio_data')\n",
        "eluvio_data='/content/eluvio_data'"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "Collecting pyunpack\n",
            "  Downloading https://files.pythonhosted.org/packages/83/29/020436b1d8e96e5f26fa282b9c3c13a3b456a36b9ea2edc87c5fed008369/pyunpack-0.2.2-py2.py3-none-any.whl\n",
            "Collecting easyprocess\n",
            "  Downloading https://files.pythonhosted.org/packages/48/3c/75573613641c90c6d094059ac28adb748560d99bd27ee6f80cce398f404e/EasyProcess-0.3-py2.py3-none-any.whl\n",
            "Collecting entrypoint2\n",
            "  Downloading https://files.pythonhosted.org/packages/8a/b0/8ef4b1d8be02448d164c52466530059d7f57218655d21309a0c4236d7454/entrypoint2-0.2.4-py3-none-any.whl\n",
            "Installing collected packages: easyprocess, entrypoint2, pyunpack\n",
            "Successfully installed easyprocess-0.3 entrypoint2-0.2.4 pyunpack-0.2.2\n",
            "Collecting patool\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/94/52243ddff508780dd2d8110964320ab4851134a55ab102285b46e740f76a/patool-1.12-py2.py3-none-any.whl (77kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 5.5MB/s \n",
            "\u001b[?25hInstalling collected packages: patool\n",
            "Successfully installed patool-1.12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceClWaX10uQb"
      },
      "source": [
        "data=spark.read.csv(eluvio_data,header=True,enforceSchema=True)\n",
        "Data=data.select(data[\"up_votes\"].cast(\"int\"),data[\"down_votes\"].cast(\"int\"),data[\"title\"],\n",
        "                data[\"over_18\"].cast(\"boolean\"),data[\"author\"]).na.drop()\n",
        "\n",
        "Data=Data.withColumn(\"over_18\",Data[\"over_18\"].cast(\"string\"))\n",
        "Data=Data.withColumn(\"paper_written\",Data[\"down_votes\"]+1)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K42Ff1f72uXz"
      },
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.sql.functions import sum\n",
        "string_index=StringIndexer(inputCol=\"over_18\",outputCol=\"over_18_index\")\n",
        "Data=string_index.fit(Data).transform(Data)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRmgSYFp2uZw"
      },
      "source": [
        "# lets see which authors got most upvotes\n",
        "\n",
        "Data_author=Data.groupby(\"author\").sum()\n",
        "Data_author=Data_author.withColumn(\"% of likes\",(Data_author[\"sum(up_votes)\"]*100)/(Data_author.select(sum(\"sum(up_votes)\")).collect()[0][0]))\n",
        "Most_up_voted_authors=Data_author.orderBy(Data_author[\"sum(up_votes)\"].desc())"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hug9VRuF2ub2",
        "outputId": "84a22f4c-2f90-47f4-d6c5-b7298fe51413"
      },
      "source": [
        "# Most_up_voted_authors (in descending order)\n",
        "Most_up_voted_authors.show()\n",
        "Most_up_voted_authors.select(\"author\").head(10)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------+-------------+---------------+------------------+------------------+-------------------+\n",
            "|          author|sum(up_votes)|sum(down_votes)|sum(paper_written)|sum(over_18_index)|         % of likes|\n",
            "+----------------+-------------+---------------+------------------+------------------+-------------------+\n",
            "|     maxwellhill|      1935264|              0|              3846|               0.0|  3.421626723902172|\n",
            "|       anutensil|      1430816|              0|              4921|               0.0|  2.529741814339961|\n",
            "|      Libertatea|       832089|              0|              2106|               1.0| 1.4711677368385059|\n",
            "|   DoremusJessup|       584340|              0|              5032|               0.0| 1.0331372669801098|\n",
            "|        Wagamaga|       580121|              0|              1490|               0.0|  1.025677900636219|\n",
            "| NinjaDiscoJesus|       487903|              0|              2431|               1.0| 0.8626326658647303|\n",
            "|   madazzahatter|       428966|              0|              2503|               0.0| 0.7584296143809935|\n",
            "|          madam1|       390541|              0|              2657|               0.0| 0.6904926265251036|\n",
            "|          kulkke|       333311|              0|              1199|               0.0| 0.5893076215805992|\n",
            "|   davidreiss666|       298149|              0|              8185|               0.0|  0.527139752563324|\n",
            "|         pnewell|       296768|              0|              1553|               0.0|  0.524698087495556|\n",
            "|          nimobo|       266733|              0|              2564|               0.0|0.47159496634391895|\n",
            "|       trot-trot|       258226|              0|              1644|               0.0| 0.4565542388048154|\n",
            "|         ionised|       256159|              0|              2493|               0.0| 0.4528996973891192|\n",
            "|EightRoundsRapid|       254670|              0|              1223|               0.0|0.45026708385841213|\n",
            "|          twolf1|       230023|              0|              2922|               2.0| 0.4066901693578495|\n",
            "|          mepper|       222948|              0|               698|               1.0| 0.3941812769940129|\n",
            "|       PanAfrica|       219738|              0|              1182|               2.0| 0.3885058643455443|\n",
            "|     readerseven|       208176|              0|              3087|               6.0|0.36806377056311623|\n",
            "|     green_flash|       204882|              0|               633|               1.0|0.36223984244347274|\n",
            "+----------------+-------------+---------------+------------------+------------------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(author='maxwellhill'),\n",
              " Row(author='anutensil'),\n",
              " Row(author='Libertatea'),\n",
              " Row(author='DoremusJessup'),\n",
              " Row(author='Wagamaga'),\n",
              " Row(author='NinjaDiscoJesus'),\n",
              " Row(author='madazzahatter'),\n",
              " Row(author='madam1'),\n",
              " Row(author='kulkke'),\n",
              " Row(author='davidreiss666')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFdSgzMlw-Yx"
      },
      "source": [
        "THe top authors with most likes  are : maxwellhill,anutensil,Libertatea,DoremusJessup,Wagamaga,NinjaDiscoJesus,madazzahatter,madam1,kulkke,and davidreiss666\n",
        "\n",
        "\n",
        "> Indented block\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9ATsLlF2ueG",
        "outputId": "554eb7e1-714d-4032-eab2-f03427d948e8"
      },
      "source": [
        "# the authors with least_up votes are( ascending order of upvotes):\n",
        "Least_up_voted_authors=Data_author.orderBy(\"sum(up_votes)\")\n",
        "Least_up_voted_authors.show( )"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------+-------------+---------------+------------------+------------------+----------+\n",
            "|           author|sum(up_votes)|sum(down_votes)|sum(paper_written)|sum(over_18_index)|% of likes|\n",
            "+-----------------+-------------+---------------+------------------+------------------+----------+\n",
            "|       luciusXVII|            0|              0|                 1|               0.0|       0.0|\n",
            "|   postmastercard|            0|              0|                 1|               0.0|       0.0|\n",
            "|        justwalks|            0|              0|                 1|               0.0|       0.0|\n",
            "|          debtkid|            0|              0|                 2|               0.0|       0.0|\n",
            "|           lightP|            0|              0|                 1|               0.0|       0.0|\n",
            "|          acidity|            0|              0|                 1|               0.0|       0.0|\n",
            "|      RedditRedIt|            0|              0|                 1|               0.0|       0.0|\n",
            "|      kohli290866|            0|              0|                 1|               0.0|       0.0|\n",
            "|       rampage786|            0|              0|                 1|               0.0|       0.0|\n",
            "|         nanoha88|            0|              0|                 1|               0.0|       0.0|\n",
            "|      warrior1357|            0|              0|                 1|               0.0|       0.0|\n",
            "|         kaltman2|            0|              0|                 2|               0.0|       0.0|\n",
            "|         Terbo977|            0|              0|                 1|               0.0|       0.0|\n",
            "|          BlazeOn|            0|              0|                 1|               0.0|       0.0|\n",
            "|     mcgoobersons|            0|              0|                 1|               0.0|       0.0|\n",
            "|   TheAngryCookie|            0|              0|                 1|               0.0|       0.0|\n",
            "|nomoreshallwepart|            0|              0|                 1|               0.0|       0.0|\n",
            "|           eatsam|            0|              0|                 1|               0.0|       0.0|\n",
            "|         FoxxxyIT|            0|              0|                 1|               0.0|       0.0|\n",
            "|          cyplasm|            0|              0|                 1|               0.0|       0.0|\n",
            "+-----------------+-------------+---------------+------------------+------------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OcfvhVb2uf4",
        "outputId": "5847cbf8-282c-4e07-ea93-8faee9604e89"
      },
      "source": [
        "#Total number of authors who have 0 upvotes and their percentage representation is :\n",
        "k=Least_up_voted_authors.filter(Least_up_voted_authors[\"sum(up_votes)\"]==0).count()\n",
        "print(k)\n",
        "print(\"Percent of authors have have not got any upvotes = \"+ str((k/Least_up_voted_authors.count()) *100 )+\"%\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13639\n",
            "Percent of authors have have not got any upvotes = 16.028910565283816%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMP4i3vf25s8"
      },
      "source": [
        "upvotes_to_paper_published=Data_author.withColumn(\"likes_to_paper_ratio\",Data_author[\"sum(up_votes)\"]/Data_author[\"sum(paper_written)\"])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGfVRjRzyAEC"
      },
      "source": [
        "13639 authors, ie ( in percentage), 16.028910565283816% of authors have  got 0 upvotes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7b_BA0Y25u-",
        "outputId": "25291034-b7d8-414d-a276-8e078d800a55"
      },
      "source": [
        "upvotes_to_paper_published=upvotes_to_paper_published.orderBy(upvotes_to_paper_published[\"likes_to_paper_ratio\"].desc())\n",
        "upvotes_to_paper_published.columns\n",
        "upvotes_to_paper_published=upvotes_to_paper_published.select(['author',\n",
        " 'sum(up_votes)',\n",
        " 'sum(paper_written)',\n",
        " 'sum(over_18_index)',\n",
        " 'likes_to_paper_ratio'\n",
        "  ,'% of likes'])\n",
        "\n",
        "\n",
        "# authors with best likes to number of papers published ratio ( descending order)\n",
        "upvotes_to_paper_published.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------------+-------------+------------------+------------------+--------------------+--------------------+\n",
            "|             author|sum(up_votes)|sum(paper_written)|sum(over_18_index)|likes_to_paper_ratio|          % of likes|\n",
            "+-------------------+-------------+------------------+------------------+--------------------+--------------------+\n",
            "|   navysealassulter|        12333|                 1|               0.0|             12333.0|0.021805253642854662|\n",
            "|          seapiglet|        11288|                 1|               0.0|             11288.0| 0.01995765045978622|\n",
            "|      DawgsOnTopUGA|        10515|                 1|               0.0|             10515.0|0.018590954516712624|\n",
            "|Flamo_the_Idiot_Boy|        10289|                 1|               0.0|             10289.0|0.018191377177599256|\n",
            "| haunted_cheesecake|         9408|                 1|               0.0|              9408.0| 0.01663373277158653|\n",
            "|bendertheoffender22|         8781|                 1|               0.0|              8781.0|0.015525170861745463|\n",
            "|      crippledrejex|         8601|                 1|               0.0|              8601.0|0.015206923423513577|\n",
            "|        FlandersNed|         8446|                 1|               0.0|              8446.0|0.014932877018369454|\n",
            "|          lesseva96|         8404|                 1|               0.0|              8404.0|0.014858619282782015|\n",
            "|        sverdrupian|         8262|                 1|               0.0|              8262.0|0.014607557414843528|\n",
            "|         beachedazd|         8260|                 1|               0.0|              8260.0|0.014604021332196506|\n",
            "|   NectarCollecting|         8152|                 1|               0.0|              8152.0|0.014413072869257375|\n",
            "|            Bjartur|         8115|                 1|               0.0|              8115.0|0.014347655340287488|\n",
            "|        YoungPotato|         8104|                 1|               0.0|              8104.0|0.014328206885728872|\n",
            "|           naybones|         8031|                 1|               0.0|              8031.0|0.014199139869112608|\n",
            "|      house_of_kunt|         8012|                 1|               0.0|              8012.0| 0.01416554708396591|\n",
            "|       Grizzly-Slim|         7899|                 1|               0.0|              7899.0|0.013965758414409226|\n",
            "|           lowbread|         7869|                 1|               0.0|              7869.0| 0.01391271717470391|\n",
            "|        Impmaster82|         7836|                 1|               0.0|              7836.0|0.013854371811028067|\n",
            "|             Aerest|         7722|                 1|               0.0|              7722.0|0.013652815100147871|\n",
            "+-------------------+-------------+------------------+------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euV8hujD1TCV"
      },
      "source": [
        "The above data frame represents author with highest up votes per paper. Most of them seems to have written just one article and got large up-votes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUq7Bmhn25w1",
        "outputId": "9ef0934f-be9a-4e11-f283-2489daae561d"
      },
      "source": [
        "#lets see the number of papers that are for adults only\n",
        "\n",
        "count_not_for_adults=Data.filter(Data[\"over_18\"]==\"false\").count()\n",
        "count_for_adults=Data.filter(Data[\"over_18\"]==\"true\").count()\n",
        "\n",
        "print(\"Total number of articles which are for adove 18 audience = \" + str(count_for_adults))\n",
        "print(\"Total number of articles which are not for adove 18 audience = \" + str(count_not_for_adults))\n",
        "\n",
        "print(\"% of articles for adults only = \" + str((count_for_adults*100)/(count_for_adults+count_not_for_adults)))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of articles which are for adove 18 audience = 315\n",
            "Total number of articles which are not for adove 18 audience = 499537\n",
            "% of articles for adults only = 0.06301865352144234\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzKADnTu1o64"
      },
      "source": [
        "This shows that the data is highly skew in terms of content for above 18 only vs contents for all audience. In fact ,only 0.06% of articles are for above 18 only"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIqIJm-e25zN",
        "outputId": "54b42e66-9c26-48d3-fdda-22b3ddb2cf29"
      },
      "source": [
        "from pyspark.sql.functions import length\n",
        "Data=Data.withColumn(\"length_of_title\",length(Data[\"title\"]))\n",
        "Data_to_be_used=Data.select([\"title\",\"over_18_index\",\"length_of_title\"])\n",
        "Data_to_be_used.show()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+-------------+---------------+\n",
            "|               title|over_18_index|length_of_title|\n",
            "+--------------------+-------------+---------------+\n",
            "|Scores killed in ...|          0.0|             33|\n",
            "|Japan resumes ref...|          0.0|             32|\n",
            "|US presses Egypt ...|          0.0|             31|\n",
            "|Jump-start econom...|          0.0|             44|\n",
            "|Council of Europe...|          0.0|             47|\n",
            "|Hay presto! Farme...|          0.0|             97|\n",
            "|Strikes, Protests...|          0.0|             59|\n",
            "|The U.N. Mismanag...|          0.0|             30|\n",
            "|Nicolas Sarkozy t...|          0.0|             41|\n",
            "|US plans for miss...|          0.0|             71|\n",
            "|Archbishop of Can...|          0.0|             95|\n",
            "|Top US Envoy: Vio...|          0.0|             53|\n",
            "|Team building flo...|          0.0|             90|\n",
            "|Migrant workers t...|          0.0|             60|\n",
            "| Sarkozy, Girlfri...|          0.0|             40|\n",
            "|Nicolas Sarkozy, ...|          0.0|             85|\n",
            "|Mass Evacuations ...|          0.0|             35|\n",
            "|Poor Haitians Res...|          0.0|             35|\n",
            "|European Commissi...|          0.0|             73|\n",
            "|Rambo banned in B...|          0.0|             21|\n",
            "+--------------------+-------------+---------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BlkWtdL252p",
        "outputId": "ad3ca4c3-2c77-4bc0-e858-227ea7cb34b8"
      },
      "source": [
        "# this shows thar irrespective of category of article, the average lengh of its title is almost the same\n",
        "Data_to_be_used.groupBy(\"over_18_index\").mean().show()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------+------------------+--------------------+\n",
            "|over_18_index|avg(over_18_index)|avg(length_of_title)|\n",
            "+-------------+------------------+--------------------+\n",
            "|          0.0|               0.0|   88.76956862054263|\n",
            "|          1.0|               1.0|   87.62222222222222|\n",
            "+-------------+------------------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsvGz4Nr3m9e"
      },
      "source": [
        "However, length of title doesnt give any insights about classifying data over_18 and under_18"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1i2W1i5l3A0g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6265303-6e6a-4b60-9acf-3d0404df1031"
      },
      "source": [
        "from pyspark.ml.feature import Tokenizer,StopWordsRemover,CountVectorizer,IDF,VectorAssembler\n",
        "from pyspark.ml.pipeline import Pipeline\n",
        "from pyspark.ml.classification import RandomForestClassifier,NaiveBayes\n",
        "\n",
        "tokenizers= Tokenizer(inputCol=\"title\",outputCol=\"tokens\")\n",
        "stop_words_removed=StopWordsRemover(inputCol=\"tokens\",outputCol=\"stop_words_removed\")\n",
        "vector_count=CountVectorizer(inputCol=\"stop_words_removed\",outputCol=\"vector_c\")\n",
        "inverse_df=IDF(inputCol=\"vector_c\",outputCol=\"idf\")\n",
        "ready_data=VectorAssembler(inputCols=[\"idf\",\"length_of_title\"],outputCol=\"feature\")\n",
        "naive_bias=NaiveBayes(featuresCol=\"feature\",labelCol=\"over_18_index\")\n",
        "\n",
        "data_to_process=Pipeline(stages=[tokenizers,stop_words_removed,vector_count,inverse_df,ready_data])\n",
        "process=data_to_process.fit(Data_to_be_used)\n",
        "final_data=process.transform(Data_to_be_used)\n",
        "train,test=final_data.randomSplit([0.8,0.2])\n",
        "\n",
        "train.printSchema()\n",
        "train.show()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- title: string (nullable = true)\n",
            " |-- over_18_index: double (nullable = false)\n",
            " |-- length_of_title: integer (nullable = true)\n",
            " |-- tokens: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- stop_words_removed: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- vector_c: vector (nullable = true)\n",
            " |-- idf: vector (nullable = true)\n",
            " |-- feature: vector (nullable = true)\n",
            "\n",
            "+--------------------+-------------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|               title|over_18_index|length_of_title|              tokens|  stop_words_removed|            vector_c|                 idf|             feature|\n",
            "+--------------------+-------------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|\t Chevron admits ...|          0.0|            214|[, , chevron, adm...|[, , chevron, adm...|(206345,[0,1,63,1...|(206345,[0,1,63,1...|(206346,[0,1,63,1...|\n",
            "|\tDespite Assuranc...|          0.0|             78|[, despite, assur...|[, despite, assur...|(206345,[0,3,13,2...|(206345,[0,3,13,2...|(206346,[0,3,13,2...|\n",
            "|\tFrench minister ...|          0.0|             59|[, french, minist...|[, french, minist...|(206345,[0,29,62,...|(206345,[0,29,62,...|(206346,[0,29,62,...|\n",
            "|\tIAEA Chief:  Isr...|          0.0|            213|[, iaea, chief:, ...|[, iaea, chief:, ...|(206345,[0,13,15,...|(206345,[0,13,15,...|(206346,[0,13,15,...|\n",
            "|\tIranian border p...|          0.0|             38|[, iranian, borde...|[, iranian, borde...|(206345,[0,6,110,...|(206345,[0,6,110,...|(206346,[0,6,110,...|\n",
            "| \t  West  Retreat...|          0.0|             39|[, , , , west, , ...|[, , , , west, , ...|(206345,[0,15,121...|(206345,[0,15,121...|(206346,[0,15,121...|\n",
            "| \t Montgomerie Du...|          0.0|             45|[, , , montgomeri...|[, , , montgomeri...|(206345,[0,198,46...|(206345,[0,198,46...|(206346,[0,198,46...|\n",
            "| \t The best plant...|          0.0|             35|[, , , the, best,...|[, , , best, plan...|(206345,[0,10,392...|(206345,[0,10,392...|(206346,[0,10,392...|\n",
            "| \tUS Congressmen ...|          0.0|            124|[, , us, congress...|[, , us, congress...|(206345,[0,5,73,8...|(206345,[0,5,73,8...|(206346,[0,5,73,8...|\n",
            "|  \t  Report: Russ...|          0.0|             58|[, , , , , report...|[, , , , , report...|(206345,[0,7,153,...|(206345,[0,7,153,...|(206346,[0,7,153,...|\n",
            "|  \t  Shahar Peer ...|          0.0|             39|[, , , , , shahar...|[, , , , , shahar...|(206345,[0,1216,1...|(206345,[0,1216,1...|(206346,[0,1216,1...|\n",
            "|  \t  US journalis...|          0.0|             44|[, , , , , us, jo...|[, , , , , us, jo...|(206345,[0,5,240,...|(206345,[0,5,240,...|(206346,[0,5,240,...|\n",
            "|                 ...|          0.0|            161|[, , , , , , , , ...|[, , , , , , , , ...|(206345,[0,38,250...|(206345,[0,38,250...|(206346,[0,38,250...|\n",
            "|     It said abou...|          0.0|             45|[, , , , , it, sa...|[, , , , , said, ...|(206345,[0,35,39,...|(206345,[0,35,39,...|(206346,[0,35,39,...|\n",
            "|    The governmen...|          0.0|             50|[, , , , the, gov...|[, , , , governme...|(206345,[0,11,840...|(206345,[0,11,840...|(206346,[0,11,840...|\n",
            "|   17,000 flights...|          0.0|             45|[, , , 17,000, fl...|[, , , 17,000, fl...|(206345,[0,107,12...|(206345,[0,107,12...|(206346,[0,107,12...|\n",
            "|   Airlines suffe...|          0.0|             54|[, , , airlines, ...|[, , , airlines, ...|(206345,[0,359,84...|(206345,[0,359,84...|(206346,[0,359,84...|\n",
            "|   Airspace lockd...|          0.0|             43|[, , , airspace, ...|[, , , airspace, ...|(206345,[0,2125,3...|(206345,[0,2125,3...|(206346,[0,2125,3...|\n",
            "|   Angry ex-girlf...|          0.0|            119|[, , , angry, ex-...|[, , , angry, ex-...|(206345,[0,126,20...|(206345,[0,126,20...|(206346,[0,126,20...|\n",
            "|   British Airway...|          0.0|             48|[, , , british, a...|[, , , british, a...|(206345,[0,61,735...|(206345,[0,61,735...|(206346,[0,61,735...|\n",
            "+--------------------+-------------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7feJ_6A83A4t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39d281a9-6e92-4d2b-bc63-cd8004964217"
      },
      "source": [
        "model=naive_bias.fit(train)\n",
        "finale=model.transform(test)\n",
        "\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "# AS ITS A SCREW DATA SET, ACCURACY IS NOT THE BEST METRIC,SO HERE WE ARE USING AREA UNDER CURVE\n",
        "\n",
        "binary_Eval=BinaryClassificationEvaluator(labelCol=\"over_18_index\",rawPredictionCol=\"rawPrediction\")\n",
        "\n",
        "print(\"Area under Curve=\" + str(binary_Eval.evaluate(finale)*100))"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Area under Curve=41.63591163612101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9lBw-XpsoGi"
      },
      "source": [
        "In the above code, though , naive bias seems fast, its area under curve is very low (42 % approx). So, we will explore other models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrlpnc_Oswp_"
      },
      "source": [
        "**In the following code, we tried to generate a RNN model to classify titles as for all people vs for people above 18 only . Though its really tough to do for because of highly skew data, we tried to have a vague idea of time taken and performance achieved**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlhPopC60uSc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f37ef55c-be2e-4be0-81c9-fc6140c57936"
      },
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words=stopwords.words('english')\n",
        "\n",
        "pseudo_label,sent=[],[]\n",
        "with open(\"/content/eluvio_data/Eluvio_DS_Challenge.csv\", 'r') as csvfile:\n",
        "  reader = csv.reader(csvfile, delimiter=',')\n",
        "  for row in reader:\n",
        "        pseudo_label.append(row[5])\n",
        "        sentence=row[4]\n",
        "        for word in stop_words:\n",
        "          word_to_be_replaced=\" \" + word + \" \"\n",
        "          sentence=sentence.replace(word_to_be_replaced, \" \")\n",
        "        sent.append(sentence)\n",
        "  \n",
        "label=[0 if pseudo_label[i]==\"False\" else 1 for i in range(len(pseudo_label))]\n",
        "\n",
        "#print(len(sent))\n"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHkjWIxfji0d"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_HwKN1-jpl9"
      },
      "source": [
        "We have 509237 sentences. So even if we take 95% of the data in train and just 5% in test, we have len(train),len(test)=483775,25461 cases. We will be using this split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JE9p-m4VkYAl"
      },
      "source": [
        "training_size=int(0.95*len(sent))\n",
        "training_labels,training_sentences=label[:training_size],sent[:training_size]\n",
        "testing_labels,testing_sentences=label[training_size:],sent[training_size:]\n"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEQJofxUmDlR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bdcc9c4-bcaa-4590-db89-d620ce86e95c"
      },
      "source": [
        "import tensorflow \n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer( oov_token=\"@%^&\")\n",
        "tokenizer.fit_on_texts(training_sentences)\n",
        "total_words_used=len(tokenizer.word_counts)\n",
        "print(total_words_used)\n",
        "\n",
        "reverse_sorted_words = sorted(tokenizer.word_index, key=tokenizer.word_index.get, reverse=False)\n",
        "\n",
        "\n",
        "# now,lets not take all words in this corpos, we will take only  80,000 most repeated words ( based on intrusion)\n",
        "tokenizer = Tokenizer( oov_token=\"@%^&\",num_words=80000)\n",
        "tokenizer.fit_on_texts(training_sentences)\n",
        "\n",
        "training_sentences_to_tokens=tokenizer.texts_to_sequences(training_sentences)\n",
        "\n",
        "# now,we will padd these generted sentences_to_token and make sure that they are of fixed length. As the length of largest statement is 300, however, the average ,\n",
        "#length is only 76 with standard deviation of 46.  We will make every padded statement to be of size of 120 and we will add 0 in \"pre\" format, ie ,for\n",
        "# smaller statements, they will start with 0 to make the total length of sentence to 120\n",
        "\n",
        "padded_train_statements=pad_sequences(sequences=training_sentences_to_tokens,maxlen=120,padding=\"pre\")\n",
        "\n",
        "train_x=padded_train_statements\n",
        "train_y=np.asarray(training_labels)\n",
        "\n",
        "test=tokenizer.texts_to_sequences(testing_sentences)\n",
        "test_x=pad_sequences(sequences=test,maxlen=120,padding=\"pre\")\n",
        "test_y=np.asarray(testing_labels)\n"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "103788\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0q6mMeWunJz"
      },
      "source": [
        "So, we can see that ,exlcuding commonly used stop words, this data used about 103788 distint wordsand the top 100 most common words are :says',\n",
        " 'china',\n",
        " 'us',\n",
        " 'new',\n",
        " 'u',\n",
        " 's',\n",
        " 'syria',\n",
        " 'russia',\n",
        " 'world',\n",
        " 'police',\n",
        " 'government',\n",
        " 'israel',\n",
        " 'iran',\n",
        " 'a',\n",
        " 'president',\n",
        " 'killed',\n",
        " 'people',\n",
        " 'state',\n",
        " 'attack',\n",
        " 'in',\n",
        " 'war',\n",
        " 'russian',\n",
        " 'military',\n",
        " 'uk',\n",
        " 'north',\n",
        " 'year',\n",
        " 'south',\n",
        " 'korea',\n",
        " 'minister',\n",
        " 'news',\n",
        " 'said',\n",
        " 'two',\n",
        " 'first',\n",
        " 'ukraine',\n",
        " 'india',\n",
        " '000',\n",
        " 'nuclear',\n",
        " 'syrian',\n",
        " 'un',\n",
        " 'years',\n",
        " 'israeli',\n",
        " 'one',\n",
        " 'al',\n",
        " 'chinese',\n",
        " 'isis',\n",
        " 'court',\n",
        " 'country',\n",
        " 'death',\n",
        " 'eu',\n",
        " 'iraq',\n",
        " 'egypt',\n",
        " 'japan',\n",
        " 'dead',\n",
        " 'may',\n",
        " 'pakistan',\n",
        " 'could',\n",
        " 'report',\n",
        " 'anti',\n",
        " 'leader',\n",
        " 'man',\n",
        " 'islamic',\n",
        " 'say',\n",
        " 'turkey',\n",
        " 'security',\n",
        " 'to',\n",
        " 'forces',\n",
        " 'saudi',\n",
        " 'is',\n",
        " 'found',\n",
        " 'attacks',\n",
        " '1',\n",
        " 'gaza',\n",
        " 'oil',\n",
        " 'british',\n",
        " 'crisis',\n",
        " 'group',\n",
        " 'rights',\n",
        " 'french',\n",
        " 'deal',\n",
        " 'million',\n",
        " 'obama',\n",
        " '2',\n",
        " 'city',\n",
        " 'france',\n",
        " 'air',\n",
        " 'former',\n",
        " 'army',\n",
        " 'it',\n",
        " 'officials',\n",
        " 'women',\n",
        " 'germany',\n",
        " 'talks',\n",
        " 'europe',\n",
        " 'foreign',\n",
        " 'international',\n",
        " 'rebels',\n",
        " 'putin',\n",
        " 'human'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38p1wXFK985_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9db1e7eb-c716-41e1-9aa8-896ae25e8230"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,Embedding,GlobalAveragePooling1D,Conv1D,LSTM,Bidirectional,Flatten\n",
        "\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Embedding(input_dim=80000,output_dim=100,input_length=120))\n",
        "model.add(Bidirectional(LSTM(64,return_sequences=True)))\n",
        "#model.add(Bidirectional(LSTM(64,return_sequences=True)))\n",
        "model.add(Bidirectional(LSTM(128)))\n",
        "model.add(Dense(128,activation=\"relu\"))\n",
        "model.add(Dense(256,activation=\"relu\"))\n",
        "model.add(Dense(1,activation=\"sigmoid\"))\n",
        "#model.summary()\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model.fit(train_x, train_y, epochs=10, validation_data=(test_x, test_y), verbose=1,batch_size=1024)\n",
        "\n",
        "pred = model.predict(test_x, batch_size=32, verbose=1)\n",
        "predicted = np.argmax(pred, axis=1)\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "corrected_prediction=[]\n",
        "for i in predicted:\n",
        "  if i ==0:\n",
        "    corrected_prediction.append(0)\n",
        "  else:\n",
        "    corrected_prediction.append(1)\n",
        "\n",
        "corrected_prediction=np.asarray(corrected_prediction).reshape(-1,1)\n",
        "\n",
        "test_y=test_y.reshape(-1,1)\n",
        "print(\"the recall obtained =\"+ \" \" + str((f1_score(test_y,predicted,average=\"micro\")*100)))\n"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "473/473 [==============================] - 103s 209ms/step - loss: 0.0441 - accuracy: 0.9994 - val_loss: 0.0059 - val_accuracy: 0.9993\n",
            "Epoch 2/10\n",
            "473/473 [==============================] - 97s 204ms/step - loss: 0.0035 - accuracy: 0.9996 - val_loss: 0.0051 - val_accuracy: 0.9994\n",
            "Epoch 3/10\n",
            "473/473 [==============================] - 97s 205ms/step - loss: 0.0018 - accuracy: 0.9997 - val_loss: 0.0072 - val_accuracy: 0.9994\n",
            "Epoch 4/10\n",
            "473/473 [==============================] - 97s 205ms/step - loss: 5.5758e-04 - accuracy: 0.9999 - val_loss: 0.0079 - val_accuracy: 0.9994\n",
            "Epoch 5/10\n",
            "473/473 [==============================] - 97s 205ms/step - loss: 2.9269e-04 - accuracy: 0.9999 - val_loss: 0.0104 - val_accuracy: 0.9993\n",
            "Epoch 6/10\n",
            "473/473 [==============================] - 97s 205ms/step - loss: 2.4193e-04 - accuracy: 0.9999 - val_loss: 0.0102 - val_accuracy: 0.9993\n",
            "Epoch 7/10\n",
            "473/473 [==============================] - 97s 205ms/step - loss: 1.1298e-04 - accuracy: 1.0000 - val_loss: 0.0095 - val_accuracy: 0.9992\n",
            "Epoch 8/10\n",
            "473/473 [==============================] - 97s 205ms/step - loss: 8.2120e-05 - accuracy: 1.0000 - val_loss: 0.0113 - val_accuracy: 0.9992\n",
            "Epoch 9/10\n",
            "473/473 [==============================] - 97s 205ms/step - loss: 6.6234e-05 - accuracy: 1.0000 - val_loss: 0.0120 - val_accuracy: 0.9992\n",
            "Epoch 10/10\n",
            "473/473 [==============================] - 97s 206ms/step - loss: 7.6392e-05 - accuracy: 1.0000 - val_loss: 0.0124 - val_accuracy: 0.9991\n",
            "796/796 [==============================] - 9s 10ms/step\n",
            "the recall obtained = 99.92930641740632\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Paq932L8pfGH"
      },
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "\n",
        "random_forest=RandomForestClassifier(featuresCol=\"feature\",labelCol=\"over_18_index\",seed=1,numTrees=50)\n",
        "\n",
        "model_random_forest=random_forest.fit(train)\n",
        "finale_random_forest=model_random_forest.transform(test)\n",
        "\n",
        "binary_Eval=BinaryClassificationEvaluator(labelCol=\"over_18_index\",rawPredictionCol=\"rawPrediction\")\n",
        "\n",
        "print(\"Area under Curve=\" + str(binary_Eval.evaluate(finale_random_forest)*100))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XUNtqWZqrlk"
      },
      "source": [
        "While using random forest, the area under curve was 71.229%, however, random forest takes lot of time. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBYRRpw8q4xt"
      },
      "source": [
        "In the following last section, we tried to make a model that automatically generats text after being trained on litrature style of a person. We took the author with highest numbers of titles and trained a model using his/her title. Then we gave model few random words and made model to generate sentences in a way, similar to writing style of author.\n",
        "\n",
        "The top author is \"davidreiss666\" and a generated 10 word sentence is :\n",
        "\n",
        "Habyarimana commander lead Ireland  airlines state media says central jail where enforcement to show eu coalition far in its peers of its gdp is\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYB5G-HbpfJb",
        "outputId": "b8550531-bae1-4a08-8469-a7856c22cf15"
      },
      "source": [
        "import tensorflow \n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "author_with_most_papers=Data_author.orderBy(Data_author[\"sum(paper_written)\"].desc())\n",
        "most_written_author=author_with_most_papers.head(1)[0][0]\n",
        "davidreiss666=Data.filter(Data[\"Author\"]==most_written_author)\n",
        "#davidreiss666.show()\n",
        "\n",
        "titles_of_top_author=davidreiss666.select([\"title\"]).collect()\n",
        "text_obtained_from_top_authors_articles=[]\n",
        "for i in range(len(titles_of_top_author)):\n",
        "  text_obtained_from_top_authors_articles.append(titles_of_top_author[i][0]) \n",
        "\n",
        "\n",
        "tokens_for_top_authors=Tokenizer(oov_token=\"@!@#@$$%$\",lower=True)\n",
        "tokens_for_top_authors.fit_on_texts(text_obtained_from_top_authors_articles)\n",
        "\n",
        "to_token_conversations_of_sentence=[]\n",
        "for i in text_obtained_from_top_authors_articles:\n",
        "  s=tokens_for_top_authors.texts_to_sequences([i])[0]\n",
        "  #print(s)\n",
        "  for j in range(1,len(s)):\n",
        "    n_gram=s[:j+1]\n",
        "    to_token_conversations_of_sentence.append(n_gram)\n",
        "\n",
        "max_length=max([len(i) for i in to_token_conversations_of_sentence])\n",
        "\n",
        "total_words=len(tokens_for_top_authors.word_index)+1\n",
        "\n",
        "\n",
        "padding_tokens_of_top_authors=pad_sequences(to_token_conversations_of_sentence,maxlen=55,padding=\"pre\")\n",
        "\n",
        "training_for_top_author_data,labels_for_top_authors=padding_tokens_of_top_authors[:,:-1],padding_tokens_of_top_authors[:,-1]\n",
        "\n",
        "labels_for_top_authors=labels_for_top_authors.reshape(-1,1)\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,Embedding,GlobalAveragePooling1D,Conv1D,LSTM,Bidirectional,Flatten\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 80, input_length=max_length-1))\n",
        "model.add(Bidirectional(LSTM(20)))\n",
        "model.add(Dense(128,activation=\"tanh\"))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.fit(training_for_top_author_data,labels_for_top_authors,batch_size=512,verbose=1,epochs=700)\n",
        "\n",
        "text_obtained_from_top_authors_articles\n",
        "\n",
        "text=\"Habyarimana commander lead Ireland \"\n",
        "\n",
        "new_title_length=20\n",
        "\n",
        "for i in range(new_title_length):\n",
        "  token=pad_sequences([tokens_for_top_authors.texts_to_sequences([text])[0]],maxlen=54,padding=\"pre\")\n",
        "\n",
        "  result=int(model.predict_classes(token,verbose=0))\n",
        "\n",
        "  generated_word=\" \"\n",
        "\n",
        "  if result in tokens_for_top_authors.index_word.keys():\n",
        "    generated_word=tokens_for_top_authors.index_word[result]\n",
        "\n",
        "\n",
        "  text=text+ \" \" + generated_word\n",
        "\n",
        "print(text)\n",
        "  \n",
        "\n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/700\n",
            "440/440 [==============================] - 57s 48ms/step - loss: 8.0716 - accuracy: 0.0356\n",
            "Epoch 2/700\n",
            "440/440 [==============================] - 21s 48ms/step - loss: 7.1338 - accuracy: 0.0588\n",
            "Epoch 3/700\n",
            "440/440 [==============================] - 21s 47ms/step - loss: 6.8368 - accuracy: 0.0823\n",
            "Epoch 4/700\n",
            "440/440 [==============================] - 22s 51ms/step - loss: 6.5378 - accuracy: 0.1066\n",
            "Epoch 5/700\n",
            "440/440 [==============================] - 22s 49ms/step - loss: 6.2957 - accuracy: 0.1224\n",
            "Epoch 6/700\n",
            "440/440 [==============================] - 22s 49ms/step - loss: 6.1278 - accuracy: 0.1321\n",
            "Epoch 7/700\n",
            "440/440 [==============================] - 22s 49ms/step - loss: 5.9695 - accuracy: 0.1429\n",
            "Epoch 8/700\n",
            "440/440 [==============================] - 22s 49ms/step - loss: 5.8284 - accuracy: 0.1528\n",
            "Epoch 9/700\n",
            "440/440 [==============================] - 22s 50ms/step - loss: 5.6904 - accuracy: 0.1612\n",
            "Epoch 10/700\n",
            "440/440 [==============================] - 23s 52ms/step - loss: 5.5730 - accuracy: 0.1681\n",
            "Epoch 11/700\n",
            "440/440 [==============================] - 23s 52ms/step - loss: 5.4597 - accuracy: 0.1748\n",
            "Epoch 12/700\n",
            "440/440 [==============================] - 22s 51ms/step - loss: 5.3615 - accuracy: 0.1789\n",
            "Epoch 13/700\n",
            "440/440 [==============================] - 21s 49ms/step - loss: 5.2550 - accuracy: 0.1872\n",
            "Epoch 14/700\n",
            "440/440 [==============================] - 22s 50ms/step - loss: 5.1494 - accuracy: 0.1945\n",
            "Epoch 15/700\n",
            "440/440 [==============================] - 22s 49ms/step - loss: 5.0684 - accuracy: 0.1984\n",
            "Epoch 16/700\n",
            "440/440 [==============================] - 22s 50ms/step - loss: 4.9836 - accuracy: 0.2064\n",
            "Epoch 17/700\n",
            "440/440 [==============================] - 22s 50ms/step - loss: 4.8904 - accuracy: 0.2140\n",
            "Epoch 18/700\n",
            "440/440 [==============================] - 21s 49ms/step - loss: 4.8002 - accuracy: 0.2219\n",
            "Epoch 19/700\n",
            "440/440 [==============================] - 22s 51ms/step - loss: 4.7277 - accuracy: 0.2288\n",
            "Epoch 20/700\n",
            "440/440 [==============================] - 21s 49ms/step - loss: 4.6495 - accuracy: 0.2355\n",
            "Epoch 21/700\n",
            "440/440 [==============================] - 22s 50ms/step - loss: 4.5718 - accuracy: 0.2427\n",
            "Epoch 22/700\n",
            "440/440 [==============================] - 22s 50ms/step - loss: 4.5011 - accuracy: 0.2479\n",
            "Epoch 23/700\n",
            "440/440 [==============================] - 22s 49ms/step - loss: 4.4196 - accuracy: 0.2561\n",
            "Epoch 24/700\n",
            "440/440 [==============================] - 22s 49ms/step - loss: 4.3538 - accuracy: 0.2642\n",
            "Epoch 25/700\n",
            "440/440 [==============================] - 22s 49ms/step - loss: 4.3033 - accuracy: 0.2666\n",
            "Epoch 26/700\n",
            "440/440 [==============================] - 22s 50ms/step - loss: 4.2237 - accuracy: 0.2757\n",
            "Epoch 27/700\n",
            "440/440 [==============================] - 22s 51ms/step - loss: 4.1701 - accuracy: 0.2799\n",
            "Epoch 28/700\n",
            "440/440 [==============================] - 22s 50ms/step - loss: 4.1129 - accuracy: 0.2868\n",
            "Epoch 29/700\n",
            "440/440 [==============================] - 22s 50ms/step - loss: 4.0504 - accuracy: 0.2920\n",
            "Epoch 30/700\n",
            "440/440 [==============================] - 23s 52ms/step - loss: 3.9843 - accuracy: 0.2995\n",
            "Epoch 31/700\n",
            "440/440 [==============================] - 22s 49ms/step - loss: 3.9332 - accuracy: 0.3039\n",
            "Epoch 32/700\n",
            "440/440 [==============================] - 21s 48ms/step - loss: 3.8675 - accuracy: 0.3135\n",
            "Epoch 33/700\n",
            "440/440 [==============================] - 22s 50ms/step - loss: 3.8223 - accuracy: 0.3182\n",
            "Epoch 34/700\n",
            "440/440 [==============================] - 23s 52ms/step - loss: 3.7761 - accuracy: 0.3227\n",
            "Epoch 35/700\n",
            "440/440 [==============================] - 23s 51ms/step - loss: 3.7214 - accuracy: 0.3296\n",
            "Epoch 36/700\n",
            "440/440 [==============================] - 22s 50ms/step - loss: 3.6648 - accuracy: 0.3362\n",
            "Epoch 37/700\n",
            "440/440 [==============================] - 21s 47ms/step - loss: 3.6159 - accuracy: 0.3424\n",
            "Epoch 38/700\n",
            "440/440 [==============================] - 21s 47ms/step - loss: 3.5747 - accuracy: 0.3478\n",
            "Epoch 39/700\n",
            "440/440 [==============================] - 23s 52ms/step - loss: 3.5238 - accuracy: 0.3551\n",
            "Epoch 40/700\n",
            "440/440 [==============================] - 22s 49ms/step - loss: 3.4686 - accuracy: 0.3616\n",
            "Epoch 41/700\n",
            "440/440 [==============================] - 22s 51ms/step - loss: 3.4287 - accuracy: 0.3677\n",
            "Epoch 42/700\n",
            "440/440 [==============================] - 22s 51ms/step - loss: 3.3736 - accuracy: 0.3737\n",
            "Epoch 43/700\n",
            "440/440 [==============================] - 22s 51ms/step - loss: 3.3423 - accuracy: 0.3776\n",
            "Epoch 44/700\n",
            "440/440 [==============================] - 22s 51ms/step - loss: 3.2917 - accuracy: 0.3831\n",
            "Epoch 45/700\n",
            "440/440 [==============================] - 23s 52ms/step - loss: 3.2635 - accuracy: 0.3883\n",
            "Epoch 46/700\n",
            "440/440 [==============================] - 23s 51ms/step - loss: 3.2194 - accuracy: 0.3940\n",
            "Epoch 47/700\n",
            "440/440 [==============================] - 22s 51ms/step - loss: 3.1800 - accuracy: 0.3991\n",
            "Epoch 48/700\n",
            "440/440 [==============================] - 23s 51ms/step - loss: 3.1363 - accuracy: 0.4064\n",
            "Epoch 49/700\n",
            "440/440 [==============================] - 22s 49ms/step - loss: 3.1011 - accuracy: 0.4115\n",
            "Epoch 50/700\n",
            "440/440 [==============================] - 22s 50ms/step - loss: 3.0604 - accuracy: 0.4173\n",
            "Epoch 51/700\n",
            "440/440 [==============================] - 21s 49ms/step - loss: 3.0329 - accuracy: 0.4208\n",
            "Epoch 52/700\n",
            "440/440 [==============================] - 21s 49ms/step - loss: 2.9993 - accuracy: 0.4247\n",
            "Epoch 53/700\n",
            "440/440 [==============================] - 21s 47ms/step - loss: 2.9551 - accuracy: 0.4321\n",
            "Epoch 54/700\n",
            "440/440 [==============================] - 21s 49ms/step - loss: 2.9263 - accuracy: 0.4355\n",
            "Epoch 55/700\n",
            "440/440 [==============================] - 22s 50ms/step - loss: 2.8957 - accuracy: 0.4409\n",
            "Epoch 56/700\n",
            "440/440 [==============================] - 23s 52ms/step - loss: 2.8602 - accuracy: 0.4449\n",
            "Epoch 57/700\n",
            "440/440 [==============================] - 22s 51ms/step - loss: 2.8302 - accuracy: 0.4514\n",
            "Epoch 58/700\n",
            "440/440 [==============================] - 22s 51ms/step - loss: 2.7966 - accuracy: 0.4566\n",
            "Epoch 59/700\n",
            "440/440 [==============================] - 22s 50ms/step - loss: 2.7724 - accuracy: 0.4604\n",
            "Epoch 60/700\n",
            "440/440 [==============================] - 22s 50ms/step - loss: 2.7427 - accuracy: 0.4641\n",
            "Epoch 61/700\n",
            "440/440 [==============================] - 22s 50ms/step - loss: 2.7078 - accuracy: 0.4698\n",
            "Epoch 62/700\n",
            "440/440 [==============================] - 22s 50ms/step - loss: 2.6899 - accuracy: 0.4745\n",
            "Epoch 63/700\n",
            "440/440 [==============================] - 22s 51ms/step - loss: 2.6571 - accuracy: 0.4774\n",
            "Epoch 64/700\n",
            "440/440 [==============================] - 22s 51ms/step - loss: 2.6266 - accuracy: 0.4835\n",
            "Epoch 65/700\n",
            "440/440 [==============================] - 23s 52ms/step - loss: 2.5959 - accuracy: 0.4891\n",
            "Epoch 66/700\n",
            "440/440 [==============================] - 22s 50ms/step - loss: 2.5774 - accuracy: 0.4916\n",
            "Epoch 67/700\n",
            "440/440 [==============================] - 22s 49ms/step - loss: 2.5446 - accuracy: 0.4967\n",
            "Epoch 68/700\n",
            "440/440 [==============================] - 22s 51ms/step - loss: 2.5278 - accuracy: 0.5000\n",
            "Epoch 69/700\n",
            "440/440 [==============================] - 22s 50ms/step - loss: 2.4986 - accuracy: 0.5039\n",
            "Epoch 70/700\n",
            "440/440 [==============================] - 22s 50ms/step - loss: 2.4704 - accuracy: 0.5084\n",
            "Epoch 71/700\n",
            "440/440 [==============================] - 22s 50ms/step - loss: 2.4503 - accuracy: 0.5127\n",
            "Epoch 72/700\n",
            "440/440 [==============================] - 22s 49ms/step - loss: 2.4249 - accuracy: 0.5176\n",
            "Epoch 73/700\n",
            "440/440 [==============================] - 23s 52ms/step - loss: 2.4065 - accuracy: 0.5183\n",
            "Epoch 74/700\n",
            "440/440 [==============================] - 22s 50ms/step - loss: 2.3865 - accuracy: 0.5224\n",
            "Epoch 75/700\n",
            "440/440 [==============================] - 22s 51ms/step - loss: 2.3662 - accuracy: 0.5265\n",
            "Epoch 76/700\n",
            "440/440 [==============================] - 21s 48ms/step - loss: 2.3392 - accuracy: 0.5315\n",
            "Epoch 77/700\n",
            "440/440 [==============================] - 18s 42ms/step - loss: 2.3200 - accuracy: 0.5338\n",
            "Epoch 78/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 2.3019 - accuracy: 0.5390\n",
            "Epoch 79/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 2.2758 - accuracy: 0.5420\n",
            "Epoch 80/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 2.2625 - accuracy: 0.5445\n",
            "Epoch 81/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 2.2389 - accuracy: 0.5494\n",
            "Epoch 82/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 2.2138 - accuracy: 0.5528\n",
            "Epoch 83/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 2.2040 - accuracy: 0.5548\n",
            "Epoch 84/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 2.1857 - accuracy: 0.5570\n",
            "Epoch 85/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 2.1606 - accuracy: 0.5629\n",
            "Epoch 86/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 2.1474 - accuracy: 0.5647\n",
            "Epoch 87/700\n",
            "440/440 [==============================] - 15s 33ms/step - loss: 2.1310 - accuracy: 0.5665\n",
            "Epoch 88/700\n",
            "440/440 [==============================] - 15s 33ms/step - loss: 2.1097 - accuracy: 0.5717\n",
            "Epoch 89/700\n",
            "440/440 [==============================] - 15s 33ms/step - loss: 2.0956 - accuracy: 0.5752\n",
            "Epoch 90/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 2.0836 - accuracy: 0.5762\n",
            "Epoch 91/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 2.0607 - accuracy: 0.5803\n",
            "Epoch 92/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 2.0488 - accuracy: 0.5829\n",
            "Epoch 93/700\n",
            "440/440 [==============================] - 15s 33ms/step - loss: 2.0302 - accuracy: 0.5869\n",
            "Epoch 94/700\n",
            "440/440 [==============================] - 15s 33ms/step - loss: 2.0135 - accuracy: 0.5892\n",
            "Epoch 95/700\n",
            "440/440 [==============================] - 15s 33ms/step - loss: 1.9993 - accuracy: 0.5918\n",
            "Epoch 96/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.9816 - accuracy: 0.5948\n",
            "Epoch 97/700\n",
            "440/440 [==============================] - 15s 33ms/step - loss: 1.9694 - accuracy: 0.5973\n",
            "Epoch 98/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.9505 - accuracy: 0.6020\n",
            "Epoch 99/700\n",
            "440/440 [==============================] - 15s 33ms/step - loss: 1.9338 - accuracy: 0.6040\n",
            "Epoch 100/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.9297 - accuracy: 0.6041\n",
            "Epoch 101/700\n",
            "440/440 [==============================] - 15s 33ms/step - loss: 1.9113 - accuracy: 0.6084\n",
            "Epoch 102/700\n",
            "440/440 [==============================] - 15s 33ms/step - loss: 1.8982 - accuracy: 0.6108\n",
            "Epoch 103/700\n",
            "440/440 [==============================] - 15s 33ms/step - loss: 1.8798 - accuracy: 0.6152\n",
            "Epoch 104/700\n",
            "440/440 [==============================] - 15s 33ms/step - loss: 1.8773 - accuracy: 0.6149\n",
            "Epoch 105/700\n",
            "440/440 [==============================] - 15s 33ms/step - loss: 1.8603 - accuracy: 0.6180\n",
            "Epoch 106/700\n",
            "440/440 [==============================] - 15s 33ms/step - loss: 1.8486 - accuracy: 0.6201\n",
            "Epoch 107/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.8339 - accuracy: 0.6239\n",
            "Epoch 108/700\n",
            "440/440 [==============================] - 15s 33ms/step - loss: 1.8159 - accuracy: 0.6260\n",
            "Epoch 109/700\n",
            "440/440 [==============================] - 15s 33ms/step - loss: 1.8044 - accuracy: 0.6282\n",
            "Epoch 110/700\n",
            "440/440 [==============================] - 15s 33ms/step - loss: 1.7960 - accuracy: 0.6289\n",
            "Epoch 111/700\n",
            "440/440 [==============================] - 15s 33ms/step - loss: 1.7819 - accuracy: 0.6309\n",
            "Epoch 112/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.7632 - accuracy: 0.6362\n",
            "Epoch 113/700\n",
            "440/440 [==============================] - 15s 33ms/step - loss: 1.7625 - accuracy: 0.6362\n",
            "Epoch 114/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.7510 - accuracy: 0.6384\n",
            "Epoch 115/700\n",
            "440/440 [==============================] - 15s 33ms/step - loss: 1.7361 - accuracy: 0.6424\n",
            "Epoch 116/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.7361 - accuracy: 0.6406\n",
            "Epoch 117/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.7196 - accuracy: 0.6444\n",
            "Epoch 118/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.7030 - accuracy: 0.6480\n",
            "Epoch 119/700\n",
            "440/440 [==============================] - 15s 33ms/step - loss: 1.7029 - accuracy: 0.6472\n",
            "Epoch 120/700\n",
            "440/440 [==============================] - 15s 33ms/step - loss: 1.6794 - accuracy: 0.6532\n",
            "Epoch 121/700\n",
            "440/440 [==============================] - 15s 33ms/step - loss: 1.6695 - accuracy: 0.6525\n",
            "Epoch 122/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.6697 - accuracy: 0.6526\n",
            "Epoch 123/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.6596 - accuracy: 0.6558\n",
            "Epoch 124/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.6381 - accuracy: 0.6605\n",
            "Epoch 125/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.6293 - accuracy: 0.6611\n",
            "Epoch 126/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.6313 - accuracy: 0.6606\n",
            "Epoch 127/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.6175 - accuracy: 0.6636\n",
            "Epoch 128/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.6060 - accuracy: 0.6651\n",
            "Epoch 129/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.5952 - accuracy: 0.6682\n",
            "Epoch 130/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.5835 - accuracy: 0.6721\n",
            "Epoch 131/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.5754 - accuracy: 0.6725\n",
            "Epoch 132/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.5655 - accuracy: 0.6746\n",
            "Epoch 133/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.5650 - accuracy: 0.6731\n",
            "Epoch 134/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.5519 - accuracy: 0.6778\n",
            "Epoch 135/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.5367 - accuracy: 0.6800\n",
            "Epoch 136/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.5364 - accuracy: 0.6800\n",
            "Epoch 137/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.5262 - accuracy: 0.6818\n",
            "Epoch 138/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.5087 - accuracy: 0.6855\n",
            "Epoch 139/700\n",
            "440/440 [==============================] - 15s 33ms/step - loss: 1.5151 - accuracy: 0.6844\n",
            "Epoch 140/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.4960 - accuracy: 0.6896\n",
            "Epoch 141/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.4938 - accuracy: 0.6886\n",
            "Epoch 142/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.4804 - accuracy: 0.6910\n",
            "Epoch 143/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.4771 - accuracy: 0.6904\n",
            "Epoch 144/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.4784 - accuracy: 0.6906\n",
            "Epoch 145/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.4614 - accuracy: 0.6943\n",
            "Epoch 146/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.4591 - accuracy: 0.6954\n",
            "Epoch 147/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.4394 - accuracy: 0.6993\n",
            "Epoch 148/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.4408 - accuracy: 0.7001\n",
            "Epoch 149/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.4294 - accuracy: 0.7009\n",
            "Epoch 150/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.4209 - accuracy: 0.7032\n",
            "Epoch 151/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.4281 - accuracy: 0.7012\n",
            "Epoch 152/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.4239 - accuracy: 0.7032\n",
            "Epoch 153/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.4069 - accuracy: 0.7055\n",
            "Epoch 154/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.3950 - accuracy: 0.7091\n",
            "Epoch 155/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.3912 - accuracy: 0.7092\n",
            "Epoch 156/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.4124 - accuracy: 0.7026\n",
            "Epoch 157/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.3833 - accuracy: 0.7106\n",
            "Epoch 158/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.3596 - accuracy: 0.7148\n",
            "Epoch 159/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.3603 - accuracy: 0.7151\n",
            "Epoch 160/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.3630 - accuracy: 0.7142\n",
            "Epoch 161/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.3554 - accuracy: 0.7150\n",
            "Epoch 162/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.3394 - accuracy: 0.7191\n",
            "Epoch 163/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.3383 - accuracy: 0.7189\n",
            "Epoch 164/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.3243 - accuracy: 0.7214\n",
            "Epoch 165/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.3259 - accuracy: 0.7209\n",
            "Epoch 166/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.3106 - accuracy: 0.7239\n",
            "Epoch 167/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.3088 - accuracy: 0.7251\n",
            "Epoch 168/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.3060 - accuracy: 0.7237\n",
            "Epoch 169/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.3012 - accuracy: 0.7265\n",
            "Epoch 170/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.3018 - accuracy: 0.7260\n",
            "Epoch 171/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.2901 - accuracy: 0.7286\n",
            "Epoch 172/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.3034 - accuracy: 0.7240\n",
            "Epoch 173/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.2808 - accuracy: 0.7282\n",
            "Epoch 174/700\n",
            "440/440 [==============================] - 15s 33ms/step - loss: 1.2725 - accuracy: 0.7328\n",
            "Epoch 175/700\n",
            "440/440 [==============================] - 15s 33ms/step - loss: 1.2622 - accuracy: 0.7339\n",
            "Epoch 176/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.2663 - accuracy: 0.7313\n",
            "Epoch 177/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.2560 - accuracy: 0.7348\n",
            "Epoch 178/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.2557 - accuracy: 0.7351\n",
            "Epoch 179/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.2456 - accuracy: 0.7376\n",
            "Epoch 180/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.2375 - accuracy: 0.7395\n",
            "Epoch 181/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.2414 - accuracy: 0.7382\n",
            "Epoch 182/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.2309 - accuracy: 0.7406\n",
            "Epoch 183/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.2191 - accuracy: 0.7434\n",
            "Epoch 184/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.2137 - accuracy: 0.7435\n",
            "Epoch 185/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.2181 - accuracy: 0.7433\n",
            "Epoch 186/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.2141 - accuracy: 0.7435\n",
            "Epoch 187/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.2070 - accuracy: 0.7462\n",
            "Epoch 188/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.2033 - accuracy: 0.7452\n",
            "Epoch 189/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.2014 - accuracy: 0.7451\n",
            "Epoch 190/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.1874 - accuracy: 0.7488\n",
            "Epoch 191/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.1851 - accuracy: 0.7497\n",
            "Epoch 192/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.1728 - accuracy: 0.7510\n",
            "Epoch 193/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.1751 - accuracy: 0.7519\n",
            "Epoch 194/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.1677 - accuracy: 0.7537\n",
            "Epoch 195/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.1671 - accuracy: 0.7533\n",
            "Epoch 196/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.1629 - accuracy: 0.7541\n",
            "Epoch 197/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.1625 - accuracy: 0.7545\n",
            "Epoch 198/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.1638 - accuracy: 0.7518\n",
            "Epoch 199/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.1499 - accuracy: 0.7564\n",
            "Epoch 200/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.1462 - accuracy: 0.7570\n",
            "Epoch 201/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.1449 - accuracy: 0.7580\n",
            "Epoch 202/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.1259 - accuracy: 0.7616\n",
            "Epoch 203/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.1332 - accuracy: 0.7601\n",
            "Epoch 204/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.1330 - accuracy: 0.7603\n",
            "Epoch 205/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.1191 - accuracy: 0.7631\n",
            "Epoch 206/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.1187 - accuracy: 0.7627\n",
            "Epoch 207/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.1066 - accuracy: 0.7662\n",
            "Epoch 208/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.1272 - accuracy: 0.7601\n",
            "Epoch 209/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.1133 - accuracy: 0.7634\n",
            "Epoch 210/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.1105 - accuracy: 0.7641\n",
            "Epoch 211/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.0968 - accuracy: 0.7679\n",
            "Epoch 212/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.0995 - accuracy: 0.7658\n",
            "Epoch 213/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.0936 - accuracy: 0.7687\n",
            "Epoch 214/700\n",
            "440/440 [==============================] - 15s 33ms/step - loss: 1.0771 - accuracy: 0.7711\n",
            "Epoch 215/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.0928 - accuracy: 0.7668\n",
            "Epoch 216/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.0950 - accuracy: 0.7670\n",
            "Epoch 217/700\n",
            "440/440 [==============================] - 15s 33ms/step - loss: 1.0847 - accuracy: 0.7694\n",
            "Epoch 218/700\n",
            "440/440 [==============================] - 15s 33ms/step - loss: 1.0774 - accuracy: 0.7712\n",
            "Epoch 219/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.0691 - accuracy: 0.7735\n",
            "Epoch 220/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.0628 - accuracy: 0.7735\n",
            "Epoch 221/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.0613 - accuracy: 0.7747\n",
            "Epoch 222/700\n",
            "440/440 [==============================] - 15s 33ms/step - loss: 1.0499 - accuracy: 0.7773\n",
            "Epoch 223/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.0877 - accuracy: 0.7663\n",
            "Epoch 224/700\n",
            "440/440 [==============================] - 15s 33ms/step - loss: 1.0579 - accuracy: 0.7750\n",
            "Epoch 225/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.0410 - accuracy: 0.7771\n",
            "Epoch 226/700\n",
            "440/440 [==============================] - 15s 33ms/step - loss: 1.0374 - accuracy: 0.7788\n",
            "Epoch 227/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.0414 - accuracy: 0.7786\n",
            "Epoch 228/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.0349 - accuracy: 0.7783\n",
            "Epoch 229/700\n",
            "440/440 [==============================] - 15s 33ms/step - loss: 1.0279 - accuracy: 0.7817\n",
            "Epoch 230/700\n",
            "440/440 [==============================] - 15s 33ms/step - loss: 1.0274 - accuracy: 0.7807\n",
            "Epoch 231/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.0385 - accuracy: 0.7785\n",
            "Epoch 232/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.0281 - accuracy: 0.7816\n",
            "Epoch 233/700\n",
            "440/440 [==============================] - 15s 33ms/step - loss: 1.0191 - accuracy: 0.7827\n",
            "Epoch 234/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.0212 - accuracy: 0.7818\n",
            "Epoch 235/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.0080 - accuracy: 0.7843\n",
            "Epoch 236/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.0042 - accuracy: 0.7857\n",
            "Epoch 237/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.0000 - accuracy: 0.7865\n",
            "Epoch 238/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.9939 - accuracy: 0.7879\n",
            "Epoch 239/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.0006 - accuracy: 0.7864\n",
            "Epoch 240/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.0144 - accuracy: 0.7818\n",
            "Epoch 241/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.9983 - accuracy: 0.7871\n",
            "Epoch 242/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.9863 - accuracy: 0.7897\n",
            "Epoch 243/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.0045 - accuracy: 0.7847\n",
            "Epoch 244/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.9835 - accuracy: 0.7899\n",
            "Epoch 245/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.9791 - accuracy: 0.7905\n",
            "Epoch 246/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.9779 - accuracy: 0.7904\n",
            "Epoch 247/700\n",
            "440/440 [==============================] - 15s 33ms/step - loss: 0.9721 - accuracy: 0.7927\n",
            "Epoch 248/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 1.0096 - accuracy: 0.7818\n",
            "Epoch 249/700\n",
            "440/440 [==============================] - 15s 33ms/step - loss: 0.9861 - accuracy: 0.7882\n",
            "Epoch 250/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.9570 - accuracy: 0.7961\n",
            "Epoch 251/700\n",
            "440/440 [==============================] - 15s 33ms/step - loss: 0.9558 - accuracy: 0.7964\n",
            "Epoch 252/700\n",
            "440/440 [==============================] - 15s 33ms/step - loss: 0.9592 - accuracy: 0.7950\n",
            "Epoch 253/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.9547 - accuracy: 0.7952\n",
            "Epoch 254/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.9570 - accuracy: 0.7933\n",
            "Epoch 255/700\n",
            "440/440 [==============================] - 15s 33ms/step - loss: 0.9537 - accuracy: 0.7949\n",
            "Epoch 256/700\n",
            "440/440 [==============================] - 15s 33ms/step - loss: 0.9526 - accuracy: 0.7955\n",
            "Epoch 257/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.9607 - accuracy: 0.7937\n",
            "Epoch 258/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.9549 - accuracy: 0.7938\n",
            "Epoch 259/700\n",
            "440/440 [==============================] - 15s 33ms/step - loss: 0.9412 - accuracy: 0.7999\n",
            "Epoch 260/700\n",
            "440/440 [==============================] - 15s 33ms/step - loss: 0.9331 - accuracy: 0.7994\n",
            "Epoch 261/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.9333 - accuracy: 0.8001\n",
            "Epoch 262/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.9423 - accuracy: 0.7987\n",
            "Epoch 263/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.9349 - accuracy: 0.7988\n",
            "Epoch 264/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.9260 - accuracy: 0.8017\n",
            "Epoch 265/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.9253 - accuracy: 0.8020\n",
            "Epoch 266/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.9264 - accuracy: 0.8004\n",
            "Epoch 267/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.9278 - accuracy: 0.8002\n",
            "Epoch 268/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.9138 - accuracy: 0.8038\n",
            "Epoch 269/700\n",
            "440/440 [==============================] - 15s 33ms/step - loss: 0.9223 - accuracy: 0.8012\n",
            "Epoch 270/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.9200 - accuracy: 0.8030\n",
            "Epoch 271/700\n",
            "440/440 [==============================] - 15s 33ms/step - loss: 0.9246 - accuracy: 0.8013\n",
            "Epoch 272/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.9138 - accuracy: 0.8031\n",
            "Epoch 273/700\n",
            "440/440 [==============================] - 15s 33ms/step - loss: 0.9046 - accuracy: 0.8059\n",
            "Epoch 274/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.9032 - accuracy: 0.8053\n",
            "Epoch 275/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.8957 - accuracy: 0.8086\n",
            "Epoch 276/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.8918 - accuracy: 0.8079\n",
            "Epoch 277/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.8922 - accuracy: 0.8084\n",
            "Epoch 278/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.8944 - accuracy: 0.8076\n",
            "Epoch 279/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.9006 - accuracy: 0.8061\n",
            "Epoch 280/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.8937 - accuracy: 0.8077\n",
            "Epoch 281/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.8926 - accuracy: 0.8085\n",
            "Epoch 282/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.8876 - accuracy: 0.8104\n",
            "Epoch 283/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.8911 - accuracy: 0.8082\n",
            "Epoch 284/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.8822 - accuracy: 0.8100\n",
            "Epoch 285/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.8737 - accuracy: 0.8129\n",
            "Epoch 286/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.8726 - accuracy: 0.8132\n",
            "Epoch 287/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.8866 - accuracy: 0.8090\n",
            "Epoch 288/700\n",
            "440/440 [==============================] - 15s 33ms/step - loss: 0.8708 - accuracy: 0.8124\n",
            "Epoch 289/700\n",
            "440/440 [==============================] - 15s 33ms/step - loss: 0.8684 - accuracy: 0.8135\n",
            "Epoch 290/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.8695 - accuracy: 0.8128\n",
            "Epoch 291/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.8734 - accuracy: 0.8131\n",
            "Epoch 292/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.8558 - accuracy: 0.8161\n",
            "Epoch 293/700\n",
            "440/440 [==============================] - 15s 33ms/step - loss: 0.8585 - accuracy: 0.8158\n",
            "Epoch 294/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.8600 - accuracy: 0.8142\n",
            "Epoch 295/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.8546 - accuracy: 0.8155\n",
            "Epoch 296/700\n",
            "440/440 [==============================] - 15s 33ms/step - loss: 0.8511 - accuracy: 0.8163\n",
            "Epoch 297/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.8476 - accuracy: 0.8181\n",
            "Epoch 298/700\n",
            "440/440 [==============================] - 15s 33ms/step - loss: 0.8433 - accuracy: 0.8192\n",
            "Epoch 299/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.8525 - accuracy: 0.8165\n",
            "Epoch 300/700\n",
            "440/440 [==============================] - 15s 33ms/step - loss: 0.8413 - accuracy: 0.8191\n",
            "Epoch 301/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.8481 - accuracy: 0.8178\n",
            "Epoch 302/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.8418 - accuracy: 0.8182\n",
            "Epoch 303/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.8354 - accuracy: 0.8208\n",
            "Epoch 304/700\n",
            "440/440 [==============================] - 15s 33ms/step - loss: 0.8455 - accuracy: 0.8162\n",
            "Epoch 305/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.8298 - accuracy: 0.8214\n",
            "Epoch 306/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.8333 - accuracy: 0.8202\n",
            "Epoch 307/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.8210 - accuracy: 0.8237\n",
            "Epoch 308/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.8237 - accuracy: 0.8224\n",
            "Epoch 309/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.8344 - accuracy: 0.8185\n",
            "Epoch 310/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.8241 - accuracy: 0.8219\n",
            "Epoch 311/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.8175 - accuracy: 0.8238\n",
            "Epoch 312/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.8183 - accuracy: 0.8237\n",
            "Epoch 313/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.8183 - accuracy: 0.8235\n",
            "Epoch 314/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.8121 - accuracy: 0.8243\n",
            "Epoch 315/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.8127 - accuracy: 0.8237\n",
            "Epoch 316/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.8080 - accuracy: 0.8260\n",
            "Epoch 317/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.8078 - accuracy: 0.8251\n",
            "Epoch 318/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.8162 - accuracy: 0.8232\n",
            "Epoch 319/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.8175 - accuracy: 0.8226\n",
            "Epoch 320/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.8086 - accuracy: 0.8256\n",
            "Epoch 321/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.8093 - accuracy: 0.8238\n",
            "Epoch 322/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.8096 - accuracy: 0.8261\n",
            "Epoch 323/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.7990 - accuracy: 0.8277\n",
            "Epoch 324/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.8020 - accuracy: 0.8258\n",
            "Epoch 325/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.7918 - accuracy: 0.8297\n",
            "Epoch 326/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.7833 - accuracy: 0.8314\n",
            "Epoch 327/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.7816 - accuracy: 0.8322\n",
            "Epoch 328/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.7880 - accuracy: 0.8303\n",
            "Epoch 329/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.7994 - accuracy: 0.8275\n",
            "Epoch 330/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.7909 - accuracy: 0.8275\n",
            "Epoch 331/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.7897 - accuracy: 0.8285\n",
            "Epoch 332/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.7802 - accuracy: 0.8313\n",
            "Epoch 333/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.7741 - accuracy: 0.8321\n",
            "Epoch 334/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.7823 - accuracy: 0.8297\n",
            "Epoch 335/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.7870 - accuracy: 0.8278\n",
            "Epoch 336/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.7734 - accuracy: 0.8316\n",
            "Epoch 337/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.7732 - accuracy: 0.8328\n",
            "Epoch 338/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.7668 - accuracy: 0.8348\n",
            "Epoch 339/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.7604 - accuracy: 0.8355\n",
            "Epoch 340/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.7697 - accuracy: 0.8341\n",
            "Epoch 341/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.7752 - accuracy: 0.8320\n",
            "Epoch 342/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.7709 - accuracy: 0.8319\n",
            "Epoch 343/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.7586 - accuracy: 0.8356\n",
            "Epoch 344/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.7594 - accuracy: 0.8349\n",
            "Epoch 345/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.8312 - accuracy: 0.8160\n",
            "Epoch 346/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.7714 - accuracy: 0.8326\n",
            "Epoch 347/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.7430 - accuracy: 0.8398\n",
            "Epoch 348/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.7492 - accuracy: 0.8386\n",
            "Epoch 349/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.7478 - accuracy: 0.8381\n",
            "Epoch 350/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.7415 - accuracy: 0.8391\n",
            "Epoch 351/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.7538 - accuracy: 0.8364\n",
            "Epoch 352/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.7525 - accuracy: 0.8358\n",
            "Epoch 353/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.7522 - accuracy: 0.8366\n",
            "Epoch 354/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.7561 - accuracy: 0.8355\n",
            "Epoch 355/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.7656 - accuracy: 0.8334\n",
            "Epoch 356/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.7385 - accuracy: 0.8409\n",
            "Epoch 357/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.7343 - accuracy: 0.8403\n",
            "Epoch 358/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.7442 - accuracy: 0.8386\n",
            "Epoch 359/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.7388 - accuracy: 0.8394\n",
            "Epoch 360/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.7371 - accuracy: 0.8395\n",
            "Epoch 361/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.7353 - accuracy: 0.8395\n",
            "Epoch 362/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.7349 - accuracy: 0.8411\n",
            "Epoch 363/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.7354 - accuracy: 0.8402\n",
            "Epoch 364/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.8030 - accuracy: 0.8206\n",
            "Epoch 365/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.7495 - accuracy: 0.8347\n",
            "Epoch 366/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.7198 - accuracy: 0.8441\n",
            "Epoch 367/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.7160 - accuracy: 0.8458\n",
            "Epoch 368/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.7197 - accuracy: 0.8445\n",
            "Epoch 369/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.7346 - accuracy: 0.8397\n",
            "Epoch 370/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.7562 - accuracy: 0.8340\n",
            "Epoch 371/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.7343 - accuracy: 0.8403\n",
            "Epoch 372/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.7210 - accuracy: 0.8436\n",
            "Epoch 373/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.7056 - accuracy: 0.8480\n",
            "Epoch 374/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.7076 - accuracy: 0.8476\n",
            "Epoch 375/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.7103 - accuracy: 0.8455\n",
            "Epoch 376/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.7175 - accuracy: 0.8432\n",
            "Epoch 377/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.7283 - accuracy: 0.8402\n",
            "Epoch 378/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.7201 - accuracy: 0.8427\n",
            "Epoch 379/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.7234 - accuracy: 0.8431\n",
            "Epoch 380/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.7325 - accuracy: 0.8385\n",
            "Epoch 381/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.7126 - accuracy: 0.8439\n",
            "Epoch 382/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6942 - accuracy: 0.8499\n",
            "Epoch 383/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.7001 - accuracy: 0.8489\n",
            "Epoch 384/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.6960 - accuracy: 0.8502\n",
            "Epoch 385/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.7370 - accuracy: 0.8377\n",
            "Epoch 386/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.7307 - accuracy: 0.8393\n",
            "Epoch 387/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.6974 - accuracy: 0.8478\n",
            "Epoch 388/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6927 - accuracy: 0.8502\n",
            "Epoch 389/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6893 - accuracy: 0.8517\n",
            "Epoch 390/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.7091 - accuracy: 0.8442\n",
            "Epoch 391/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.7046 - accuracy: 0.8455\n",
            "Epoch 392/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.7329 - accuracy: 0.8372\n",
            "Epoch 393/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6941 - accuracy: 0.8491\n",
            "Epoch 394/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6818 - accuracy: 0.8522\n",
            "Epoch 395/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6812 - accuracy: 0.8527\n",
            "Epoch 396/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6996 - accuracy: 0.8476\n",
            "Epoch 397/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.7024 - accuracy: 0.8456\n",
            "Epoch 398/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6937 - accuracy: 0.8484\n",
            "Epoch 399/700\n",
            "440/440 [==============================] - 16s 35ms/step - loss: 0.6850 - accuracy: 0.8491\n",
            "Epoch 400/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6937 - accuracy: 0.8476\n",
            "Epoch 401/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.6809 - accuracy: 0.8521\n",
            "Epoch 402/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6906 - accuracy: 0.8486\n",
            "Epoch 403/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.6899 - accuracy: 0.8487\n",
            "Epoch 404/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.6851 - accuracy: 0.8504\n",
            "Epoch 405/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.6822 - accuracy: 0.8516\n",
            "Epoch 406/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6830 - accuracy: 0.8508\n",
            "Epoch 407/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.6782 - accuracy: 0.8516\n",
            "Epoch 408/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6755 - accuracy: 0.8515\n",
            "Epoch 409/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.6752 - accuracy: 0.8525\n",
            "Epoch 410/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6797 - accuracy: 0.8509\n",
            "Epoch 411/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6761 - accuracy: 0.8518\n",
            "Epoch 412/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6746 - accuracy: 0.8532\n",
            "Epoch 413/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6706 - accuracy: 0.8534\n",
            "Epoch 414/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6646 - accuracy: 0.8536\n",
            "Epoch 415/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6791 - accuracy: 0.8499\n",
            "Epoch 416/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6751 - accuracy: 0.8524\n",
            "Epoch 417/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6752 - accuracy: 0.8523\n",
            "Epoch 418/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6690 - accuracy: 0.8552\n",
            "Epoch 419/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6665 - accuracy: 0.8544\n",
            "Epoch 420/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6509 - accuracy: 0.8584\n",
            "Epoch 421/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6715 - accuracy: 0.8524\n",
            "Epoch 422/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6646 - accuracy: 0.8540\n",
            "Epoch 423/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6573 - accuracy: 0.8556\n",
            "Epoch 424/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.6780 - accuracy: 0.8502\n",
            "Epoch 425/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6620 - accuracy: 0.8550\n",
            "Epoch 426/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6562 - accuracy: 0.8558\n",
            "Epoch 427/700\n",
            "440/440 [==============================] - 15s 34ms/step - loss: 0.6496 - accuracy: 0.8585\n",
            "Epoch 428/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6613 - accuracy: 0.8545\n",
            "Epoch 429/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6874 - accuracy: 0.8464\n",
            "Epoch 430/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6571 - accuracy: 0.8561\n",
            "Epoch 431/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6462 - accuracy: 0.8592\n",
            "Epoch 432/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.6432 - accuracy: 0.8607\n",
            "Epoch 433/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6657 - accuracy: 0.8542\n",
            "Epoch 434/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6460 - accuracy: 0.8589\n",
            "Epoch 435/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6455 - accuracy: 0.8592\n",
            "Epoch 436/700\n",
            "440/440 [==============================] - 16s 35ms/step - loss: 0.6386 - accuracy: 0.8599\n",
            "Epoch 437/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6490 - accuracy: 0.8583\n",
            "Epoch 438/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.6604 - accuracy: 0.8559\n",
            "Epoch 439/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6440 - accuracy: 0.8579\n",
            "Epoch 440/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6463 - accuracy: 0.8578\n",
            "Epoch 441/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6453 - accuracy: 0.8589\n",
            "Epoch 442/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6561 - accuracy: 0.8558\n",
            "Epoch 443/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6499 - accuracy: 0.8573\n",
            "Epoch 444/700\n",
            "440/440 [==============================] - 16s 35ms/step - loss: 0.6582 - accuracy: 0.8547\n",
            "Epoch 445/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6380 - accuracy: 0.8600\n",
            "Epoch 446/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6290 - accuracy: 0.8635\n",
            "Epoch 447/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6457 - accuracy: 0.8584\n",
            "Epoch 448/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6372 - accuracy: 0.8601\n",
            "Epoch 449/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6306 - accuracy: 0.8629\n",
            "Epoch 450/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6404 - accuracy: 0.8590\n",
            "Epoch 451/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.6492 - accuracy: 0.8560\n",
            "Epoch 452/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6295 - accuracy: 0.8638\n",
            "Epoch 453/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6276 - accuracy: 0.8623\n",
            "Epoch 454/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6262 - accuracy: 0.8628\n",
            "Epoch 455/700\n",
            "440/440 [==============================] - 16s 35ms/step - loss: 0.6427 - accuracy: 0.8580\n",
            "Epoch 456/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6309 - accuracy: 0.8617\n",
            "Epoch 457/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6161 - accuracy: 0.8655\n",
            "Epoch 458/700\n",
            "440/440 [==============================] - 16s 35ms/step - loss: 0.6289 - accuracy: 0.8612\n",
            "Epoch 459/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6416 - accuracy: 0.8573\n",
            "Epoch 460/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6367 - accuracy: 0.8603\n",
            "Epoch 461/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6337 - accuracy: 0.8593\n",
            "Epoch 462/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6164 - accuracy: 0.8656\n",
            "Epoch 463/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6204 - accuracy: 0.8641\n",
            "Epoch 464/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6270 - accuracy: 0.8619\n",
            "Epoch 465/700\n",
            "440/440 [==============================] - 16s 35ms/step - loss: 0.6231 - accuracy: 0.8638\n",
            "Epoch 466/700\n",
            "440/440 [==============================] - 16s 35ms/step - loss: 0.6302 - accuracy: 0.8613\n",
            "Epoch 467/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6262 - accuracy: 0.8614\n",
            "Epoch 468/700\n",
            "440/440 [==============================] - 16s 35ms/step - loss: 0.6104 - accuracy: 0.8660\n",
            "Epoch 469/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6182 - accuracy: 0.8638\n",
            "Epoch 470/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6156 - accuracy: 0.8642\n",
            "Epoch 471/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6220 - accuracy: 0.8626\n",
            "Epoch 472/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6218 - accuracy: 0.8624\n",
            "Epoch 473/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6178 - accuracy: 0.8637\n",
            "Epoch 474/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6253 - accuracy: 0.8616\n",
            "Epoch 475/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6185 - accuracy: 0.8639\n",
            "Epoch 476/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6202 - accuracy: 0.8628\n",
            "Epoch 477/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6013 - accuracy: 0.8687\n",
            "Epoch 478/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.5940 - accuracy: 0.8704\n",
            "Epoch 479/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6038 - accuracy: 0.8674\n",
            "Epoch 480/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.5963 - accuracy: 0.8688\n",
            "Epoch 481/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6027 - accuracy: 0.8665\n",
            "Epoch 482/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6390 - accuracy: 0.8571\n",
            "Epoch 483/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6122 - accuracy: 0.8642\n",
            "Epoch 484/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6165 - accuracy: 0.8628\n",
            "Epoch 485/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.6177 - accuracy: 0.8632\n",
            "Epoch 486/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.5999 - accuracy: 0.8687\n",
            "Epoch 487/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6010 - accuracy: 0.8685\n",
            "Epoch 488/700\n",
            "440/440 [==============================] - 16s 35ms/step - loss: 0.5868 - accuracy: 0.8720\n",
            "Epoch 489/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.5893 - accuracy: 0.8696\n",
            "Epoch 490/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.5964 - accuracy: 0.8695\n",
            "Epoch 491/700\n",
            "440/440 [==============================] - 16s 35ms/step - loss: 0.5993 - accuracy: 0.8673\n",
            "Epoch 492/700\n",
            "440/440 [==============================] - 16s 35ms/step - loss: 0.5982 - accuracy: 0.8679\n",
            "Epoch 493/700\n",
            "440/440 [==============================] - 16s 35ms/step - loss: 0.6077 - accuracy: 0.8650\n",
            "Epoch 494/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6081 - accuracy: 0.8647\n",
            "Epoch 495/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6227 - accuracy: 0.8610\n",
            "Epoch 496/700\n",
            "440/440 [==============================] - 16s 35ms/step - loss: 0.6190 - accuracy: 0.8616\n",
            "Epoch 497/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6906 - accuracy: 0.8446\n",
            "Epoch 498/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5973 - accuracy: 0.8680\n",
            "Epoch 499/700\n",
            "440/440 [==============================] - 16s 35ms/step - loss: 0.5835 - accuracy: 0.8723\n",
            "Epoch 500/700\n",
            "440/440 [==============================] - 16s 35ms/step - loss: 0.5816 - accuracy: 0.8734\n",
            "Epoch 501/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.5924 - accuracy: 0.8689\n",
            "Epoch 502/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.5849 - accuracy: 0.8711\n",
            "Epoch 503/700\n",
            "440/440 [==============================] - 16s 35ms/step - loss: 0.5889 - accuracy: 0.8705\n",
            "Epoch 504/700\n",
            "440/440 [==============================] - 16s 35ms/step - loss: 0.6018 - accuracy: 0.8675\n",
            "Epoch 505/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5823 - accuracy: 0.8722\n",
            "Epoch 506/700\n",
            "440/440 [==============================] - 16s 35ms/step - loss: 0.5827 - accuracy: 0.8716\n",
            "Epoch 507/700\n",
            "440/440 [==============================] - 16s 35ms/step - loss: 0.5728 - accuracy: 0.8749\n",
            "Epoch 508/700\n",
            "440/440 [==============================] - 16s 35ms/step - loss: 0.6211 - accuracy: 0.8611\n",
            "Epoch 509/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.6186 - accuracy: 0.8615\n",
            "Epoch 510/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6136 - accuracy: 0.8628\n",
            "Epoch 511/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5949 - accuracy: 0.8674\n",
            "Epoch 512/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5814 - accuracy: 0.8720\n",
            "Epoch 513/700\n",
            "440/440 [==============================] - 16s 35ms/step - loss: 0.5674 - accuracy: 0.8754\n",
            "Epoch 514/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.5641 - accuracy: 0.8773\n",
            "Epoch 515/700\n",
            "440/440 [==============================] - 16s 35ms/step - loss: 0.6023 - accuracy: 0.8650\n",
            "Epoch 516/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.5981 - accuracy: 0.8656\n",
            "Epoch 517/700\n",
            "440/440 [==============================] - 16s 35ms/step - loss: 0.6351 - accuracy: 0.8557\n",
            "Epoch 518/700\n",
            "440/440 [==============================] - 16s 35ms/step - loss: 0.6348 - accuracy: 0.8557\n",
            "Epoch 519/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5719 - accuracy: 0.8750\n",
            "Epoch 520/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.5616 - accuracy: 0.8782\n",
            "Epoch 521/700\n",
            "440/440 [==============================] - 16s 35ms/step - loss: 0.5640 - accuracy: 0.8768\n",
            "Epoch 522/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.5790 - accuracy: 0.8722\n",
            "Epoch 523/700\n",
            "440/440 [==============================] - 16s 35ms/step - loss: 0.5691 - accuracy: 0.8747\n",
            "Epoch 524/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.5650 - accuracy: 0.8767\n",
            "Epoch 525/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.5704 - accuracy: 0.8728\n",
            "Epoch 526/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.5751 - accuracy: 0.8723\n",
            "Epoch 527/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6059 - accuracy: 0.8638\n",
            "Epoch 528/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.6764 - accuracy: 0.8421\n",
            "Epoch 529/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.5949 - accuracy: 0.8686\n",
            "Epoch 530/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5535 - accuracy: 0.8793\n",
            "Epoch 531/700\n",
            "440/440 [==============================] - 16s 35ms/step - loss: 0.5436 - accuracy: 0.8820\n",
            "Epoch 532/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5500 - accuracy: 0.8808\n",
            "Epoch 533/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.5587 - accuracy: 0.8780\n",
            "Epoch 534/700\n",
            "440/440 [==============================] - 16s 35ms/step - loss: 0.5741 - accuracy: 0.8732\n",
            "Epoch 535/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.5868 - accuracy: 0.8685\n",
            "Epoch 536/700\n",
            "440/440 [==============================] - 15s 35ms/step - loss: 0.5915 - accuracy: 0.8665\n",
            "Epoch 537/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5718 - accuracy: 0.8736\n",
            "Epoch 538/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5762 - accuracy: 0.8716\n",
            "Epoch 539/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5472 - accuracy: 0.8797\n",
            "Epoch 540/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5583 - accuracy: 0.8779\n",
            "Epoch 541/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5935 - accuracy: 0.8666\n",
            "Epoch 542/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5843 - accuracy: 0.8689\n",
            "Epoch 543/700\n",
            "440/440 [==============================] - 16s 35ms/step - loss: 0.5676 - accuracy: 0.8744\n",
            "Epoch 544/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5486 - accuracy: 0.8798\n",
            "Epoch 545/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5542 - accuracy: 0.8779\n",
            "Epoch 546/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5726 - accuracy: 0.8721\n",
            "Epoch 547/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5780 - accuracy: 0.8702\n",
            "Epoch 548/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5683 - accuracy: 0.8734\n",
            "Epoch 549/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5591 - accuracy: 0.8774\n",
            "Epoch 550/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5740 - accuracy: 0.8727\n",
            "Epoch 551/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5622 - accuracy: 0.8750\n",
            "Epoch 552/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5905 - accuracy: 0.8685\n",
            "Epoch 553/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5589 - accuracy: 0.8768\n",
            "Epoch 554/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5571 - accuracy: 0.8765\n",
            "Epoch 555/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5552 - accuracy: 0.8772\n",
            "Epoch 556/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5637 - accuracy: 0.8741\n",
            "Epoch 557/700\n",
            "440/440 [==============================] - 16s 37ms/step - loss: 0.5511 - accuracy: 0.8786\n",
            "Epoch 558/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5561 - accuracy: 0.8764\n",
            "Epoch 559/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5409 - accuracy: 0.8804\n",
            "Epoch 560/700\n",
            "440/440 [==============================] - 16s 35ms/step - loss: 0.5563 - accuracy: 0.8769\n",
            "Epoch 561/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5420 - accuracy: 0.8804\n",
            "Epoch 562/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5517 - accuracy: 0.8777\n",
            "Epoch 563/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5738 - accuracy: 0.8712\n",
            "Epoch 564/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5664 - accuracy: 0.8729\n",
            "Epoch 565/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5429 - accuracy: 0.8798\n",
            "Epoch 566/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5351 - accuracy: 0.8819\n",
            "Epoch 567/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5450 - accuracy: 0.8791\n",
            "Epoch 568/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5809 - accuracy: 0.8688\n",
            "Epoch 569/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5709 - accuracy: 0.8718\n",
            "Epoch 570/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5707 - accuracy: 0.8712\n",
            "Epoch 571/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5463 - accuracy: 0.8789\n",
            "Epoch 572/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5384 - accuracy: 0.8824\n",
            "Epoch 573/700\n",
            "440/440 [==============================] - 16s 35ms/step - loss: 0.5296 - accuracy: 0.8844\n",
            "Epoch 574/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5288 - accuracy: 0.8832\n",
            "Epoch 575/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.6214 - accuracy: 0.8575\n",
            "Epoch 576/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5824 - accuracy: 0.8676\n",
            "Epoch 577/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5363 - accuracy: 0.8820\n",
            "Epoch 578/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5185 - accuracy: 0.8880\n",
            "Epoch 579/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5520 - accuracy: 0.8768\n",
            "Epoch 580/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5298 - accuracy: 0.8829\n",
            "Epoch 581/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5234 - accuracy: 0.8851\n",
            "Epoch 582/700\n",
            "440/440 [==============================] - 16s 35ms/step - loss: 0.5377 - accuracy: 0.8800\n",
            "Epoch 583/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5451 - accuracy: 0.8778\n",
            "Epoch 584/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5435 - accuracy: 0.8779\n",
            "Epoch 585/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5333 - accuracy: 0.8818\n",
            "Epoch 586/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5708 - accuracy: 0.8707\n",
            "Epoch 587/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5577 - accuracy: 0.8750\n",
            "Epoch 588/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5608 - accuracy: 0.8731\n",
            "Epoch 589/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5316 - accuracy: 0.8829\n",
            "Epoch 590/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5221 - accuracy: 0.8861\n",
            "Epoch 591/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5261 - accuracy: 0.8843\n",
            "Epoch 592/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5255 - accuracy: 0.8846\n",
            "Epoch 593/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5496 - accuracy: 0.8768\n",
            "Epoch 594/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5579 - accuracy: 0.8752\n",
            "Epoch 595/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5416 - accuracy: 0.8784\n",
            "Epoch 596/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5240 - accuracy: 0.8847\n",
            "Epoch 597/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5256 - accuracy: 0.8831\n",
            "Epoch 598/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5350 - accuracy: 0.8797\n",
            "Epoch 599/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5431 - accuracy: 0.8776\n",
            "Epoch 600/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5289 - accuracy: 0.8818\n",
            "Epoch 601/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5365 - accuracy: 0.8797\n",
            "Epoch 602/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5595 - accuracy: 0.8745\n",
            "Epoch 603/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5825 - accuracy: 0.8659\n",
            "Epoch 604/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5254 - accuracy: 0.8832\n",
            "Epoch 605/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5063 - accuracy: 0.8890\n",
            "Epoch 606/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5044 - accuracy: 0.8890\n",
            "Epoch 607/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5219 - accuracy: 0.8845\n",
            "Epoch 608/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5327 - accuracy: 0.8819\n",
            "Epoch 609/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5241 - accuracy: 0.8829\n",
            "Epoch 610/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5381 - accuracy: 0.8790\n",
            "Epoch 611/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5359 - accuracy: 0.8788\n",
            "Epoch 612/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5370 - accuracy: 0.8784\n",
            "Epoch 613/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5214 - accuracy: 0.8841\n",
            "Epoch 614/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5457 - accuracy: 0.8775\n",
            "Epoch 615/700\n",
            "440/440 [==============================] - 16s 35ms/step - loss: 0.5360 - accuracy: 0.8799\n",
            "Epoch 616/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5245 - accuracy: 0.8822\n",
            "Epoch 617/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5259 - accuracy: 0.8824\n",
            "Epoch 618/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5172 - accuracy: 0.8858\n",
            "Epoch 619/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5265 - accuracy: 0.8831\n",
            "Epoch 620/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5176 - accuracy: 0.8844\n",
            "Epoch 621/700\n",
            "440/440 [==============================] - 16s 35ms/step - loss: 0.5177 - accuracy: 0.8840\n",
            "Epoch 622/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5116 - accuracy: 0.8863\n",
            "Epoch 623/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5227 - accuracy: 0.8836\n",
            "Epoch 624/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5298 - accuracy: 0.8805\n",
            "Epoch 625/700\n",
            "440/440 [==============================] - 16s 35ms/step - loss: 0.5224 - accuracy: 0.8830\n",
            "Epoch 626/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5247 - accuracy: 0.8829\n",
            "Epoch 627/700\n",
            "440/440 [==============================] - 16s 35ms/step - loss: 0.5524 - accuracy: 0.8723\n",
            "Epoch 628/700\n",
            "440/440 [==============================] - 16s 35ms/step - loss: 0.5187 - accuracy: 0.8844\n",
            "Epoch 629/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5112 - accuracy: 0.8864\n",
            "Epoch 630/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5161 - accuracy: 0.8860\n",
            "Epoch 631/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5516 - accuracy: 0.8743\n",
            "Epoch 632/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5065 - accuracy: 0.8874\n",
            "Epoch 633/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.4965 - accuracy: 0.8912\n",
            "Epoch 634/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5002 - accuracy: 0.8898\n",
            "Epoch 635/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5025 - accuracy: 0.8884\n",
            "Epoch 636/700\n",
            "440/440 [==============================] - 16s 35ms/step - loss: 0.5389 - accuracy: 0.8779\n",
            "Epoch 637/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5595 - accuracy: 0.8707\n",
            "Epoch 638/700\n",
            "440/440 [==============================] - 16s 35ms/step - loss: 0.5236 - accuracy: 0.8819\n",
            "Epoch 639/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.4979 - accuracy: 0.8906\n",
            "Epoch 640/700\n",
            "440/440 [==============================] - 16s 35ms/step - loss: 0.5042 - accuracy: 0.8873\n",
            "Epoch 641/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.4978 - accuracy: 0.8903\n",
            "Epoch 642/700\n",
            "440/440 [==============================] - 16s 37ms/step - loss: 0.5765 - accuracy: 0.8653\n",
            "Epoch 643/700\n",
            "440/440 [==============================] - 16s 37ms/step - loss: 0.5374 - accuracy: 0.8767\n",
            "Epoch 644/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5253 - accuracy: 0.8813\n",
            "Epoch 645/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.4953 - accuracy: 0.8900\n",
            "Epoch 646/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.4827 - accuracy: 0.8941\n",
            "Epoch 647/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.4886 - accuracy: 0.8931\n",
            "Epoch 648/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5000 - accuracy: 0.8873\n",
            "Epoch 649/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5147 - accuracy: 0.8835\n",
            "Epoch 650/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5107 - accuracy: 0.8845\n",
            "Epoch 651/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5751 - accuracy: 0.8657\n",
            "Epoch 652/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5279 - accuracy: 0.8792\n",
            "Epoch 653/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.4921 - accuracy: 0.8912\n",
            "Epoch 654/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.4965 - accuracy: 0.8897\n",
            "Epoch 655/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.4900 - accuracy: 0.8911\n",
            "Epoch 656/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5184 - accuracy: 0.8828\n",
            "Epoch 657/700\n",
            "440/440 [==============================] - 16s 35ms/step - loss: 0.5164 - accuracy: 0.8826\n",
            "Epoch 658/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5011 - accuracy: 0.8880\n",
            "Epoch 659/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.4868 - accuracy: 0.8930\n",
            "Epoch 660/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5004 - accuracy: 0.8885\n",
            "Epoch 661/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5184 - accuracy: 0.8815\n",
            "Epoch 662/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5067 - accuracy: 0.8863\n",
            "Epoch 663/700\n",
            "440/440 [==============================] - 16s 35ms/step - loss: 0.4907 - accuracy: 0.8901\n",
            "Epoch 664/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.4924 - accuracy: 0.8892\n",
            "Epoch 665/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5043 - accuracy: 0.8872\n",
            "Epoch 666/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5138 - accuracy: 0.8831\n",
            "Epoch 667/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5021 - accuracy: 0.8877\n",
            "Epoch 668/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5242 - accuracy: 0.8817\n",
            "Epoch 669/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.4959 - accuracy: 0.8898\n",
            "Epoch 670/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5213 - accuracy: 0.8799\n",
            "Epoch 671/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5264 - accuracy: 0.8804\n",
            "Epoch 672/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5068 - accuracy: 0.8850\n",
            "Epoch 673/700\n",
            "440/440 [==============================] - 16s 35ms/step - loss: 0.4986 - accuracy: 0.8886\n",
            "Epoch 674/700\n",
            "440/440 [==============================] - 16s 35ms/step - loss: 0.4863 - accuracy: 0.8921\n",
            "Epoch 675/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.4810 - accuracy: 0.8927\n",
            "Epoch 676/700\n",
            "440/440 [==============================] - 16s 35ms/step - loss: 0.4836 - accuracy: 0.8921\n",
            "Epoch 677/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.4971 - accuracy: 0.8878\n",
            "Epoch 678/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5048 - accuracy: 0.8843\n",
            "Epoch 679/700\n",
            "440/440 [==============================] - 16s 37ms/step - loss: 0.5015 - accuracy: 0.8859\n",
            "Epoch 680/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5037 - accuracy: 0.8869\n",
            "Epoch 681/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.4861 - accuracy: 0.8909\n",
            "Epoch 682/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.4939 - accuracy: 0.8897\n",
            "Epoch 683/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5415 - accuracy: 0.8744\n",
            "Epoch 684/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5035 - accuracy: 0.8866\n",
            "Epoch 685/700\n",
            "440/440 [==============================] - 16s 37ms/step - loss: 0.4769 - accuracy: 0.8935\n",
            "Epoch 686/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5008 - accuracy: 0.8867\n",
            "Epoch 687/700\n",
            "440/440 [==============================] - 16s 37ms/step - loss: 0.4888 - accuracy: 0.8905\n",
            "Epoch 688/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.4803 - accuracy: 0.8929\n",
            "Epoch 689/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5037 - accuracy: 0.8859\n",
            "Epoch 690/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5031 - accuracy: 0.8862\n",
            "Epoch 691/700\n",
            "440/440 [==============================] - 16s 37ms/step - loss: 0.4903 - accuracy: 0.8890\n",
            "Epoch 692/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.4894 - accuracy: 0.8900\n",
            "Epoch 693/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.4998 - accuracy: 0.8860\n",
            "Epoch 694/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5025 - accuracy: 0.8863\n",
            "Epoch 695/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5144 - accuracy: 0.8836\n",
            "Epoch 696/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.5039 - accuracy: 0.8847\n",
            "Epoch 697/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.4990 - accuracy: 0.8868\n",
            "Epoch 698/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.4693 - accuracy: 0.8962\n",
            "Epoch 699/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.4798 - accuracy: 0.8929\n",
            "Epoch 700/700\n",
            "440/440 [==============================] - 16s 36ms/step - loss: 0.4801 - accuracy: 0.8931\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Habyarimana commander lead Ireland  s family s soldiers sentenced on anti mafia city appears prosecutor said should be held to his presidential election chatting\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}